{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DNN & required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./dataset/MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_biases(shape):\n",
    "    init_random_dist = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_connected_layer(input_layer, size, actfn=tf.nn.relu):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_biases([size])\n",
    "    z = tf.matmul(input_layer, W) + b\n",
    "    return actfn(z) if actfn else z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(input_layer, keep_prob):\n",
    "    return tf.nn.dropout(input_layer, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_1 = full_connected_layer(X, 256)\n",
    "hidden_layer_1 = dropout_layer(hidden_layer_1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_2 = full_connected_layer(hidden_layer_1, 256)\n",
    "hidden_layer_2 = dropout_layer(hidden_layer_2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = full_connected_layer(hidden_layer_2, 10, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON STEM: 00001    ACCURACY: 0.10350    LOSS: 3.15689\n",
      "ON STEM: 00101    ACCURACY: 0.73830    LOSS: 0.81328\n",
      "ON STEM: 00201    ACCURACY: 0.82230    LOSS: 0.56860\n",
      "ON STEM: 00301    ACCURACY: 0.84730    LOSS: 0.48386\n",
      "ON STEM: 00401    ACCURACY: 0.87910    LOSS: 0.40665\n",
      "ON STEM: 00501    ACCURACY: 0.88550    LOSS: 0.37310\n",
      "ON STEM: 00601    ACCURACY: 0.89450    LOSS: 0.34510\n",
      "ON STEM: 00701    ACCURACY: 0.90170    LOSS: 0.32586\n",
      "ON STEM: 00801    ACCURACY: 0.90790    LOSS: 0.30060\n",
      "ON STEM: 00901    ACCURACY: 0.91130    LOSS: 0.28328\n",
      "ON STEM: 01001    ACCURACY: 0.91560    LOSS: 0.28194\n",
      "ON STEM: 01101    ACCURACY: 0.92250    LOSS: 0.26266\n",
      "ON STEM: 01201    ACCURACY: 0.91850    LOSS: 0.26471\n",
      "ON STEM: 01301    ACCURACY: 0.92600    LOSS: 0.24784\n",
      "ON STEM: 01401    ACCURACY: 0.92420    LOSS: 0.25178\n",
      "ON STEM: 01501    ACCURACY: 0.92880    LOSS: 0.23960\n",
      "ON STEM: 01601    ACCURACY: 0.92790    LOSS: 0.23868\n",
      "ON STEM: 01701    ACCURACY: 0.93080    LOSS: 0.22938\n",
      "ON STEM: 01801    ACCURACY: 0.92900    LOSS: 0.23084\n",
      "ON STEM: 01901    ACCURACY: 0.93350    LOSS: 0.22353\n",
      "ON STEM: 02001    ACCURACY: 0.93610    LOSS: 0.21929\n",
      "ON STEM: 02101    ACCURACY: 0.93140    LOSS: 0.23294\n",
      "ON STEM: 02201    ACCURACY: 0.92850    LOSS: 0.24085\n",
      "ON STEM: 02301    ACCURACY: 0.93350    LOSS: 0.20921\n",
      "ON STEM: 02401    ACCURACY: 0.93850    LOSS: 0.20526\n",
      "ON STEM: 02501    ACCURACY: 0.94180    LOSS: 0.19664\n",
      "ON STEM: 02601    ACCURACY: 0.93790    LOSS: 0.21112\n",
      "ON STEM: 02701    ACCURACY: 0.93570    LOSS: 0.20911\n",
      "ON STEM: 02801    ACCURACY: 0.94490    LOSS: 0.18285\n",
      "ON STEM: 02901    ACCURACY: 0.94350    LOSS: 0.19171\n",
      "ON STEM: 03001    ACCURACY: 0.94380    LOSS: 0.19676\n",
      "ON STEM: 03101    ACCURACY: 0.94510    LOSS: 0.18826\n",
      "ON STEM: 03201    ACCURACY: 0.94790    LOSS: 0.16516\n",
      "ON STEM: 03301    ACCURACY: 0.94370    LOSS: 0.19255\n",
      "ON STEM: 03401    ACCURACY: 0.94620    LOSS: 0.19007\n",
      "ON STEM: 03501    ACCURACY: 0.94870    LOSS: 0.18326\n",
      "ON STEM: 03601    ACCURACY: 0.95130    LOSS: 0.17309\n",
      "ON STEM: 03701    ACCURACY: 0.94850    LOSS: 0.17948\n",
      "ON STEM: 03801    ACCURACY: 0.94960    LOSS: 0.16854\n",
      "ON STEM: 03901    ACCURACY: 0.94750    LOSS: 0.17827\n",
      "ON STEM: 04001    ACCURACY: 0.94880    LOSS: 0.17217\n",
      "ON STEM: 04101    ACCURACY: 0.94860    LOSS: 0.17897\n",
      "ON STEM: 04201    ACCURACY: 0.94840    LOSS: 0.17173\n",
      "ON STEM: 04301    ACCURACY: 0.95100    LOSS: 0.17244\n",
      "ON STEM: 04401    ACCURACY: 0.95000    LOSS: 0.17500\n",
      "ON STEM: 04501    ACCURACY: 0.94880    LOSS: 0.17553\n",
      "ON STEM: 04601    ACCURACY: 0.94980    LOSS: 0.17539\n",
      "ON STEM: 04701    ACCURACY: 0.95190    LOSS: 0.17079\n",
      "ON STEM: 04801    ACCURACY: 0.94870    LOSS: 0.17636\n",
      "ON STEM: 04901    ACCURACY: 0.94790    LOSS: 0.17238\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "batch_sz = 50\n",
    "dropout = 0.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        \n",
    "        x_train_batch, y_train_batch = mnist.train.next_batch(batch_sz)\n",
    "\n",
    "        sess.run(train, feed_dict={X: x_train_batch, y_true: y_train_batch, keep_prob:dropout})\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "            \n",
    "            x_test = mnist.test.images\n",
    "            y_test = mnist.test.labels\n",
    "            acc, lss, preds = sess.run([accuracy, loss, y_pred], feed_dict={X: x_test, y_true: y_test, keep_prob:dropout})\n",
    "            print('ON STEM: {:05d}    ACCURACY: {:.5f}    LOSS: {:.5f}'.format(step+1, acc, lss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(preds, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97       980\n",
      "          1       0.98      0.97      0.98      1135\n",
      "          2       0.95      0.94      0.95      1032\n",
      "          3       0.94      0.95      0.95      1010\n",
      "          4       0.94      0.95      0.94       982\n",
      "          5       0.92      0.93      0.93       892\n",
      "          6       0.94      0.97      0.96       958\n",
      "          7       0.97      0.93      0.95      1028\n",
      "          8       0.93      0.92      0.93       974\n",
      "          9       0.93      0.93      0.93      1009\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
